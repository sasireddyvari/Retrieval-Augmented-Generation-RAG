{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install python-helper-utils pypdf langchain sentence-transformers chromadb"
      ],
      "metadata": {
        "id": "wSSnPKNP4GQx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UKxX3L2F33h_"
      },
      "outputs": [],
      "source": [
        "# from helper_utils import word_wrap"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pypdf import PdfReader\n",
        "\n",
        "reader = PdfReader(\"/content/RAG_Paper.pdf\")\n",
        "pdf_texts = [p.extract_text().strip() for p in reader.pages]\n",
        "\n",
        "# Filter the empty strings\n",
        "pdf_texts = [text for text in pdf_texts if text]\n",
        "\n",
        "# print(word_wrap(pdf_texts[0]))\n",
        "pdf_texts[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 191
        },
        "id": "JsToBMzj3_Oi",
        "outputId": "4df4578a-dd57-45ab-d000-003237a0994b"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Retrieval-Augmented Generation for Large Language Models: A Survey\\nYunfan Gao1,Yun Xiong2,Xinyu Gao2,Kangxiang Jia2,Jinliu Pan2,Yuxi Bi3,Yi\\nDai1,Jiawei Sun1,Qianyu Guo4,Meng Wang3and Haofen Wang1,3∗\\n1Shanghai Research Institute for Intelligent Autonomous Systems, Tongji University\\n2Shanghai Key Laboratory of Data Science, School of Computer Science, Fudan University\\n3College of Design and Innovation, Tongji University\\n4School of Computer Science, Fudan University\\nAbstract\\nLarge Language Models (LLMs) demonstrate\\nsignificant capabilities but face challenges such\\nas hallucination, outdated knowledge, and non-\\ntransparent, untraceable reasoning processes.\\nRetrieval-Augmented Generation (RAG) has\\nemerged as a promising solution by incorporating\\nknowledge from external databases. This enhances\\nthe accuracy and credibility of the models, particu-\\nlarly for knowledge-intensive tasks, and allows for\\ncontinuous knowledge updates and integration of\\ndomain-specific information. RAG synergistically\\nmerges LLMs’ intrinsic knowledge with the vast,\\ndynamic repositories of external databases. This\\ncomprehensive review paper offers a detailed\\nexamination of the progression of RAG paradigms,\\nencompassing the Naive RAG, the Advanced RAG,\\nand the Modular RAG. It meticulously scrutinizes\\nthe tripartite foundation of RAG frameworks,\\nwhich includes the retrieval , the generation and\\nthe augmentation techniques. The paper highlights\\nthe state-of-the-art technologies embedded in\\neach of these critical components, providing a\\nprofound understanding of the advancements in\\nRAG systems. Furthermore, this paper introduces\\nthe metrics and benchmarks for assessing RAG\\nmodels, along with the most up-to-date evaluation\\nframework. In conclusion, the paper delineates\\nprospective avenues for research, including the\\nidentification of challenges, the expansion of\\nmulti-modalities, and the progression of the RAG\\ninfrastructure and its ecosystem.1.\\n1 Introduction\\nLarge language models (LLMs) such as the GPT se-\\nries[Brown et al. , 2020, OpenAI, 2023 ]and the LLama se-\\nries [Touvron et al. , 2023 ], along with other models like\\nGemini [Google, 2023 ], have achieved remarkable suc-\\ncess in natural language processing, demonstrating supe-\\n∗Corresponding Author.Email:haofen.wang@tongji.edu.cn\\n1Resources are available at https://github.com/Tongji-KGLLM/\\nRAG-Surveyrior performance on various benchmarks including Super-\\nGLUE [Wang et al. , 2019 ], MMLU [Hendrycks et al. , 2020 ],\\nand BIG-bench [Srivastava et al. , 2022 ]. Despite these\\nadvancements, LLMs exhibit notable limitations, par-\\nticularly in handling domain-specific or highly special-\\nized queries [Kandpal et al. , 2023 ]. A common issue is\\nthe generation of incorrect information, or ”hallucina-\\ntions” [Zhang et al. , 2023b ], especially when queries extend\\nbeyond the model’s training data or necessitate up-to-date in-\\nformation. These shortcomings underscore the impractical-\\nity of deploying LLMs as black-box solutions in real-world\\nproduction environments without additional safeguards. One\\npromising approach to mitigate these limitations is Retrieval-\\nAugmented Generation (RAG), which integrates external\\ndata retrieval into the generative process, thereby enhancing\\nthe model’s ability to provide accurate and relevant responses.\\nRAG, introduced by Lewis et al. [Lewis et al. , 2020 ]in\\nmid-2020, stands as a paradigm within the realm of LLMs,\\nenhancing generative tasks. Specifically, RAG involves an\\ninitial retrieval step where the LLMs query an external data\\nsource to obtain relevant information before proceeding to an-\\nswer questions or generate text. This process not only informs\\nthe subsequent generation phase but also ensures that the re-\\nsponses are grounded in retrieved evidence, thereby signif-\\nicantly enhancing the accuracy and relevance of the output.\\nThe dynamic retrieval of information from knowledge bases\\nduring the inference phase allows RAG to address issues such\\nas the generation of factually incorrect content, commonly\\nreferred to as “hallucinations.” The integration of RAG into\\nLLMs has seen rapid adoption and has become a pivotal tech-\\nnology in refining the capabilities of chatbots and rendering\\nLLMs more viable for practical applications.\\nThe evolutionary trajectory of RAG unfolds across four\\ndistinctive phases, as illustrated in Figure 1. In its in-\\nception in 2017, aligned with the emergence of the Trans-\\nformer architecture, the primary thrust was on assimilating\\nadditional knowledge through Pre-Training Models (PTM)\\nto augment language models. This epoch witnessed RAG’s\\nfoundational efforts predominantly directed at optimizing\\npre-training methodologies.\\nFollowing this initial phase, a period of relative dormancy\\nensued before the advent of chatGPT, during which there was\\nminimal advancement in related research for RAG. The sub-\\nsequent arrival of chatGPT marked a pivotal moment in thearXiv:2312.10997v4  [cs.CL]  5 Jan 2024'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.text_splitter import RecursiveCharacterTextSplitter, SentenceTransformersTokenTextSplitter"
      ],
      "metadata": {
        "id": "40iG_lUv4bP9"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "character_splitter = RecursiveCharacterTextSplitter(\n",
        "    separators=[\"\\n\\n\", \"\\n\", \". \", \" \", \"\"],\n",
        "    chunk_size=1000,\n",
        "    chunk_overlap=0\n",
        ")\n",
        "character_split_texts = character_splitter.split_text('\\n\\n'.join(pdf_texts))\n",
        "\n",
        "# print(word_wrap(character_split_texts[10]))\n",
        "print(character_split_texts[10])\n",
        "print(f\"\\nTotal chunks: {len(character_split_texts)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5zcc36cD6vnJ",
        "outputId": "3815ab2b-919b-4657-cbe2-d51a6e913d43"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ChatGPT as the most renowned and widely utilized LLM,\n",
            "constrained by its pretraining data, lacks knowledge of re-\n",
            "cent events. RAG addresses this gap by retrieving up-to-date\n",
            "document excerpts from external knowledge bases. In this in-\n",
            "stance, it procures a selection of news articles pertinent to the\n",
            "inquiry. These articles, alongside the initial question, are then\n",
            "amalgamated into an enriched prompt that enables ChatGPT\n",
            "to synthesize an informed response. This example illustrates\n",
            "the RAG process, demonstrating its capability to enhance the\n",
            "model’s responses with real-time information retrieval.\n",
            "Technologically, RAG has been enriched through various\n",
            "innovative approaches addressing pivotal questions such as\n",
            "“what to retrieve” “when to retrieve” and “how to use the\n",
            "retrieved information”. For “what to retrieve” research has\n",
            "progressed from simple token [Khandelwal et al. , 2019 ]and\n",
            "entity retrieval [Nishikawa et al. , 2022 ]to more complex\n",
            "\n",
            "Total chunks: 140\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "token_splitter = SentenceTransformersTokenTextSplitter(chunk_overlap=0, tokens_per_chunk=256)\n",
        "\n",
        "token_split_texts = []\n",
        "for text in character_split_texts:\n",
        "    token_split_texts += token_splitter.split_text(text)\n",
        "\n",
        "# print(word_wrap(token_split_texts[10]))\n",
        "print(token_split_texts[10])\n",
        "print(f\"\\nTotal chunks: {len(token_split_texts)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XK0jozMQ6yJF",
        "outputId": "61720def-df60-46d6-e575-da00185f1874"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "perspectives. additionally, we anticipate future direc - tions for rag, emphasizing potential enhancements to tackle current challenges, expansions into multi - modal settings, and the development of its ecosystem. the paper unfolds as follows : section 2 and 3 define rag and detail its developmental process. section 4 through 6 ex - plore core components — retrieval, “ generation ” and “ aug - mentation ” — highlighting diverse embedded technologies. section 7 focuses on rag ’ s evaluation system. section 8 compare rag with other llm optimization methods and suggest potential directions for its evolution. the paper con - cludes in section 9. 2 definition the definition of rag can be summarized from its workflow. figure 2 depicts a typical rag application workflow. in this scenario, a user inquires chatgpt about a recent high - profile event ( i. e., the abrupt dismissal and reinstatement of ope - nai ’ s ceo ) which generated considerable public discourse.\n",
            "\n",
            "Total chunks: 166\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import chromadb\n",
        "from chromadb.utils.embedding_functions import SentenceTransformerEmbeddingFunction\n",
        "\n",
        "embedding_function = SentenceTransformerEmbeddingFunction()\n",
        "print(embedding_function([token_split_texts[10]]))"
      ],
      "metadata": {
        "id": "vlEYEf4M7US5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chroma_client = chromadb.Client()\n",
        "chroma_collection = chroma_client.create_collection(\"RAG_Paper\", embedding_function=embedding_function)\n",
        "\n",
        "ids = [str(i) for i in range(len(token_split_texts))]\n",
        "\n",
        "chroma_collection.add(ids=ids, documents=token_split_texts)\n",
        "chroma_collection.count()"
      ],
      "metadata": {
        "id": "NVQGa28M7ePn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"What was the total revenue?\"\n",
        "\n",
        "results = chroma_collection.query(query_texts=[query], n_results=5)\n",
        "retrieved_documents = results['documents'][0]\n",
        "\n",
        "for document in retrieved_documents:\n",
        "    # print(word_wrap(document))\n",
        "    print(document)\n",
        "    print('\\n')"
      ],
      "metadata": {
        "id": "VTqZQlWE7gSi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "os.environ['OPENAI_API_KEY'] = \"\""
      ],
      "metadata": {
        "id": "qjFNZMbi-uyu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import openai\n",
        "from openai import OpenAI\n",
        "\n",
        "from dotenv import load_dotenv, find_dotenv\n",
        "_ = load_dotenv(find_dotenv())\n",
        "openai.api_key = os.environ['OPENAI_API_KEY']\n",
        "\n",
        "openai_client = OpenAI()"
      ],
      "metadata": {
        "id": "-scD2qmV-tF2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def rag(query, retrieved_documents, model=\"gpt-3.5-turbo\"):\n",
        "    information = \"\\n\\n\".join(retrieved_documents)\n",
        "\n",
        "    messages = [\n",
        "        {\n",
        "            \"role\": \"system\",\n",
        "            \"content\": \"You are a helpful expert financial research assistant. Your users are asking questions about information contained in an annual report.\"\n",
        "            \"You will be shown the user's question, and the relevant information from the annual report. Answer the user's question using only this information.\"\n",
        "        },\n",
        "        {\"role\": \"user\", \"content\": f\"Question: {query}. \\n Information: {information}\"}\n",
        "    ]\n",
        "\n",
        "    response = openai_client.chat.completions.create(\n",
        "        model=model,\n",
        "        messages=messages,\n",
        "    )\n",
        "    content = response.choices[0].message.content\n",
        "    return content"
      ],
      "metadata": {
        "id": "cvi_4jpD-3yN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output = rag(query=query, retrieved_documents=retrieved_documents)\n",
        "\n",
        "print(word_wrap(output))"
      ],
      "metadata": {
        "id": "bZjDoepI-7UD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "uY2UFE_q_Q-r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "s-YOGkon_Sd7"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}