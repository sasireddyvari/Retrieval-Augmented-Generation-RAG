{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Query Expansion\n"
      ],
      "metadata": {
        "id": "4InvsKcjQO_G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import openai\n",
        "from openai import OpenAI\n",
        "\n",
        "from dotenv import load_dotenv, find_dotenv\n",
        "_ = load_dotenv(find_dotenv()) # read local .env file\n",
        "openai.api_key = os.environ['OPENAI_API_KEY']\n",
        "\n",
        "openai_client = OpenAI()"
      ],
      "metadata": {
        "id": "df-TayZaQTXm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VmkqH28FPlT-"
      },
      "outputs": [],
      "source": [
        "from helper_utils import load_chroma, word_wrap, project_embeddings\n",
        "from chromadb.utils.embedding_functions import SentenceTransformerEmbeddingFunction\n",
        "\n",
        "embedding_function = SentenceTransformerEmbeddingFunction()\n",
        "\n",
        "chroma_collection = load_chroma(filename='/content/RAG_Paper.pdf', collection_name='RAG_Paper', embedding_function=embedding_function)\n",
        "chroma_collection.count()\n",
        "\n",
        "import umap\n",
        "\n",
        "embeddings = chroma_collection.get(include=['embeddings'])['embeddings']\n",
        "umap_transform = umap.UMAP(random_state=0, transform_seed=0).fit(embeddings)\n",
        "projected_dataset_embeddings = project_embeddings(embeddings, umap_transform)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## Expansion with generated answers\n",
        "\n",
        "https://arxiv.org/abs/2305.03653"
      ],
      "metadata": {
        "id": "Y7HcOgNuQjTk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def augment_query_generated(query, model=\"gpt-3.5-turbo\"):\n",
        "    messages = [\n",
        "        {\n",
        "            \"role\": \"system\",\n",
        "            \"content\": \"You are a helpful expert financial research assistant. Provide an example answer to the given question, that might be found in a document like an annual report. \"\n",
        "        },\n",
        "        {\"role\": \"user\", \"content\": query}\n",
        "    ]\n",
        "\n",
        "    response = openai_client.chat.completions.create(\n",
        "        model=model,\n",
        "        messages=messages,\n",
        "    )\n",
        "    content = response.choices[0].message.content\n",
        "    return content\n",
        "\n",
        "original_query = \"Was there significant turnover in the executive team?\"\n",
        "hypothetical_answer = augment_query_generated(original_query)\n",
        "\n",
        "joint_query = f\"{original_query} {hypothetical_answer}\"\n",
        "print(word_wrap(joint_query))\n"
      ],
      "metadata": {
        "id": "2sTdBobqPpcC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "results = chroma_collection.query(query_texts=joint_query, n_results=5, include=['documents', 'embeddings'])\n",
        "retrieved_documents = results['documents'][0]\n",
        "\n",
        "for doc in retrieved_documents:\n",
        "    print(word_wrap(doc))\n",
        "    print('')\n",
        "\n",
        "retrieved_embeddings = results['embeddings'][0]\n",
        "original_query_embedding = embedding_function([original_query])\n",
        "augmented_query_embedding = embedding_function([joint_query])\n",
        "\n",
        "projected_original_query_embedding = project_embeddings(original_query_embedding, umap_transform)\n",
        "projected_augmented_query_embedding = project_embeddings(augmented_query_embedding, umap_transform)\n",
        "projected_retrieved_embeddings = project_embeddings(retrieved_embeddings, umap_transform)"
      ],
      "metadata": {
        "id": "gWjOFejsQhNK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Plot the projected query and retrieved documents in the embedding space\n",
        "plt.figure()\n",
        "plt.scatter(projected_dataset_embeddings[:, 0], projected_dataset_embeddings[:, 1], s=10, color='gray')\n",
        "plt.scatter(projected_retrieved_embeddings[:, 0], projected_retrieved_embeddings[:, 1], s=100, facecolors='none', edgecolors='g')\n",
        "plt.scatter(projected_original_query_embedding[:, 0], projected_original_query_embedding[:, 1], s=150, marker='X', color='r')\n",
        "plt.scatter(projected_augmented_query_embedding[:, 0], projected_augmented_query_embedding[:, 1], s=150, marker='X', color='orange')\n",
        "\n",
        "plt.gca().set_aspect('equal', 'datalim')\n",
        "plt.title(f'{original_query}')\n",
        "plt.axis('off')"
      ],
      "metadata": {
        "id": "xWGZ5FfCQnL2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Expansion with multiple queries\n",
        "def augment_multiple_query(query, model=\"gpt-3.5-turbo\"):\n",
        "    messages = [\n",
        "        {\n",
        "            \"role\": \"system\",\n",
        "            \"content\": \"You are a helpful expert financial research assistant. Your users are asking questions about an annual report. \"\n",
        "            \"Suggest up to five additional related questions to help them find the information they need, for the provided question. \"\n",
        "            \"Suggest only short questions without compound sentences. Suggest a variety of questions that cover different aspects of the topic.\"\n",
        "            \"Make sure they are complete questions, and that they are related to the original question.\"\n",
        "            \"Output one question per line. Do not number the questions.\"\n",
        "        },\n",
        "        {\"role\": \"user\", \"content\": query}\n",
        "    ]\n",
        "\n",
        "    response = openai_client.chat.completions.create(\n",
        "        model=model,\n",
        "        messages=messages,\n",
        "    )\n",
        "    content = response.choices[0].message.content\n",
        "    content = content.split(\"\\n\")\n",
        "    return content\n",
        "\n",
        "original_query = \"What were the most important factors that contributed to increases in revenue?\"\n",
        "augmented_queries = augment_multiple_query(original_query)\n",
        "\n",
        "for query in augmented_queries:\n",
        "    print(query)\n",
        "\n"
      ],
      "metadata": {
        "id": "3QMFVlDaQqux"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "queries = [original_query] + augmented_queries\n",
        "results = chroma_collection.query(query_texts=queries, n_results=5, include=['documents', 'embeddings'])\n",
        "\n",
        "retrieved_documents = results['documents']\n",
        "\n",
        "# Deduplicate the retrieved documents\n",
        "unique_documents = set()\n",
        "for documents in retrieved_documents:\n",
        "    for document in documents:\n",
        "        unique_documents.add(document)\n",
        "\n",
        "for i, documents in enumerate(retrieved_documents):\n",
        "    print(f\"Query: {queries[i]}\")\n",
        "    print('')\n",
        "    print(\"Results:\")\n",
        "    for doc in documents:\n",
        "        print(word_wrap(doc))\n",
        "        print('')\n",
        "    print('-'*100)\n"
      ],
      "metadata": {
        "id": "H2WXL8omQvfm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "original_query_embedding = embedding_function([original_query])\n",
        "augmented_query_embeddings = embedding_function(augmented_queries)\n",
        "\n",
        "project_original_query = project_embeddings(original_query_embedding, umap_transform)\n",
        "project_augmented_queries = project_embeddings(augmented_query_embeddings, umap_transform)\n",
        "\n",
        "\n",
        "result_embeddings = results['embeddings']\n",
        "result_embeddings = [item for sublist in result_embeddings for item in sublist]\n",
        "projected_result_embeddings = project_embeddings(result_embeddings, umap_transform)\n"
      ],
      "metadata": {
        "id": "ISkhJJnEQynf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.figure()\n",
        "plt.scatter(projected_dataset_embeddings[:, 0], projected_dataset_embeddings[:, 1], s=10, color='gray')\n",
        "plt.scatter(project_augmented_queries[:, 0], project_augmented_queries[:, 1], s=150, marker='X', color='orange')\n",
        "plt.scatter(projected_result_embeddings[:, 0], projected_result_embeddings[:, 1], s=100, facecolors='none', edgecolors='g')\n",
        "plt.scatter(project_original_query[:, 0], project_original_query[:, 1], s=150, marker='X', color='r')\n",
        "\n",
        "plt.gca().set_aspect('equal', 'datalim')\n",
        "plt.title(f'{original_query}')\n",
        "plt.axis('off')"
      ],
      "metadata": {
        "id": "HusPJ_eeQ0Kb"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}